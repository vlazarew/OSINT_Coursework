\section{Описание практической части}
\label{sec:Chapter4} \index{Chapter4}
\subsection{Описание выбранного инструментария}
Работа была написана на языке Python, основной фреймворк для сбора данных - Scrapy, так как эта библиотека позволяет
гибко настраивать параметры запросов, их обработку, генерацию cookie-файлов \cite{scrapyBook}. В качестве базы данных
выступает MongoDB, поскольку она хранит данные в формате JSON-подобных документов \cite{mongoDBBook}. 

\par
Поскольку поиск в поисковых сервисах и поиск в социальной сети LinkedIn отличается по концепции и настройке пауков Scrapy, то
они были выделены в 2 различных проекта.

\subsubsection{Архитектура работы сборщиков в поисковых сервисах}
\par
Диаграмма классов приведена на рис. 7. (А рисунок то где!!!)

\begin{figure}[H]
    \center{\includegraphics[height=6cm,keepaspectratio]{pictures/gzlogo.png}}
    \caption{Диаграмма классов сборщик в поисковых сервисах.}
    \label{ris:image}
\end{figure}

Система включает следующие 9 классов:
\begin{itemize}
    \item FetchSpider - позволяет собирать все документы, изображения и html-код страницы;  
    \item AbstractSearchSpider - содержит общие метода генерации запросов, обхода страниц и сбора данных с них;
    \item DuckDuckGoSearchSpider - реализует конструктор запуска сборщика для поискового сервиса DuckDuckGo и несколько 
    специфичных констант, таких как шаблон url с query и CSS-селектор найденных ссылок;
    \item GoogleSearchSpider - реализует конструктор запуска сборщика для поискового сервиса Google и несколько 
    специфичных констант, таких как шаблон url с query и CSS-селектор найденных ссылок;
    \item YahooSearch - реализует конструктор запуска сборщика для поискового сервиса Yahoo и несколько 
    специфичных констант, таких как шаблон url с query и CSS-селектор найденных ссылок;
    \item YandexSearch - реализует конструктор запуска сборщика для поискового сервиса Yandex и несколько 
    специфичных констант, таких как шаблон url с query и CSS-селектор найденных ссылок, настройки прокси;
    \item GoogleSearchApiSpider - реализует сборщик для поискового сервиса Google, который будет
    производить сбор с помощью Google API Search;
    \item GoogleAPICredentialsDownloaderMiddleware - данный класс производит неким проводником между Scrapy Engine и GoogleSearchApiSpider
    , в нем идет выбор API-ключа по стратегии "выбери тот ключ, у которого осталось наибольшее количество запросов" и обработка 429 ошибки (
        случай, когда API-ключ неожиданно превысил лимит использований и его необходимо признать невалидным, и запустить запрос 
        с новым ключом);
    \item SplashFilesPipeline - выкачивает все файлы, которые были получены в ходе сбора, если отобранная ссылка была ссылкой 
    не на html-страницу.
\end{itemize}


\subsubsection{Архитектура работы сборщиков в социальной сети LinkedIn}
\par
Диаграмма классов приведена на рис. 6. (А рисунок то где!!!)

\begin{figure}[H]
    \center{\includegraphics[height=6cm,keepaspectratio]{pictures/gzlogo.png}}
    \caption{Диаграмма классов сборщик в социальной сети LinkedIn.}
    \label{ris:image}
\end{figure}

Система включает следующие n классов:
\begin{itemize}
    \item поиск и сбор с помощью навигации по атрибутам html-кода страницы и извлечение информации из атрибутов:
    \begin{itemize}
        \item 
    \end{itemize}
    \item поиск и сбор с помощью закрытого LinkedIn API:
    \begin{itemize}
        \item 
    \end{itemize}  
\end{itemize}